Automated Product Review Summarization using Generative AI + RAG

A fully automated Retrieval-Augmented Generation (RAG) pipeline designed to summarize large volumes of customer product reviews using GPT-based LLMs, OpenAI embeddings, and ChromaDB.
This system extracts key customer sentiments, product issues, and feature insightsâ€”reducing manual review time and improving decision-making for product teams.

ğŸ“Œ Project Overview

Retail businesses receive thousands of customer reviews that are time-consuming to read and analyze.
This project solves that by building a context-aware summarization engine that combines:

Semantic Retrieval (ChromaDB + OpenAI Embeddings)

LLM Summarization (GPT-4/GPT-3.5)

BM25 Hybrid Search (optional)

ROUGE/BLEU evaluation metrics

The RAG model retrieves only the most relevant reviews and feeds them into GPT to generate accurate, concise, sentiment-aligned summaries.

ğŸ§  Architecture
Raw Reviews â†’ Preprocessing â†’ Embeddings (OpenAI) â†’ Vector Store (ChromaDB)
                     â†“
          Semantic Retrieval (Top-k Reviews)
                     â†“
        RAG Pipeline (LangChain + GPT Models)
                     â†“
       Final Summary (Sentiment-Aware, Concise)
                     â†“
       Evaluation (ROUGE/BLEU Metrics)

ğŸ”¥ Key Features

âœ” Automatic summarization of large review datasets
âœ” RAG-enabled hybrid search (ChromaDB + BM25)
âœ” GPT-based sentiment-aware summarization
âœ” Indexed vector database for fast retrieval
âœ” Streamlit UI for live summarization
âœ” FastAPI endpoint for integration
âœ” ROUGE/BLEU metric evaluation

ğŸ›  Tech Stack
Component	Tools
Language	Python
LLM	GPT-4 / GPT-3.5
Embeddings	OpenAI Embeddings 3 Small
Vector DB	ChromaDB / FAISS
Framework	LangChain
Retrieval	Semantic + BM25
Evaluation	ROUGE, BLEU
Deployment	Docker, FastAPI, Streamlit
ğŸ“ Project Structure
src/
â”‚â”€â”€ rag/
â”‚   â”œâ”€â”€ build_index.py        # Create embeddings + ChromaDB vectorstore
â”‚   â”œâ”€â”€ query_rag.py          # Retrieval + GPT summarization logic
â”‚   â”œâ”€â”€ eval.py               # ROUGE/BLEU evaluation script
â”‚â”€â”€ api/
â”‚   â””â”€â”€ api.py                # FastAPI endpoint for API use
â”‚â”€â”€ ui/
â”‚   â””â”€â”€ app_streamlit.py      # Streamlit UI for demo
data/
â”‚â”€â”€ raw/
â”‚   â””â”€â”€ reviews.csv           # Source reviews
â”‚â”€â”€ processed/
â”‚   â””â”€â”€ reviews.pkl           # Cleaned/processed reviews
models/
â”‚â”€â”€ bm25.pkl                  # BM25 index
vectorstore/
â”‚â”€â”€ chroma_db/                # Chroma vector database
requirements.txt
Dockerfile
README.md

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Add API Key (.env file)
OPENAI_API_KEY=your_key_here

3ï¸âƒ£ Build the Index (Embeddings + Vectorstore)
python src/rag/build_index.py

4ï¸âƒ£ Run Streamlit App
streamlit run src/ui/app_streamlit.py

5ï¸âƒ£ Run FastAPI
uvicorn src.api.api:app --reload

ğŸ§ª Evaluation (ROUGE/BLEU)

Run:

python src/rag/eval.py


The script outputs:

ROUGE-1

ROUGE-L

BLEU score

Summary vs. Reference comparison

ğŸ“Š Business Impact

âœ” Reduced manual review analysis time by 50%
âœ” Enabled faster product insight generation
âœ” Improved accuracy of customer sentiment interpretation
âœ” Helped product teams identify top issues and feature requests quickly

ğŸ™‹â€â™€ï¸ Author

Shriya Nair
Data Scientist | GenAI | RAG | NLP
